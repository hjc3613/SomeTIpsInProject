import tensorflow as tf
import keras
from keras import backend as K
import torch
import torch.nn as nn
import numpy as np

def sigmod(x):
    return 1.0/(1+np.exp(-x))


# 单标签二分类分类例子
logits = np.random.randn(6, 15, 1)
labels = np.random.randint(2, size=(6, 15, 1)).astype(np.double)

y_predict=sigmod(logits)

loss_1=logits*(1-labels)+np.log(1+np.exp(-logits))
print('公式写的函数\n',loss_1)
print('------------------')
print('tensorflow中的函数\n')
print(tf.nn.sigmoid_cross_entropy_with_logits(labels=labels, logits=logits))
print('------------------')
print('torch.nn.BCLossWithLogits\n')
print(nn.BCEWithLogitsLoss(reduce=False)(torch.from_numpy(logits), torch.from_numpy(labels)))

################## 总结 ######################################################
'''
pytorch BCEWithLogitsLoss == tensorflow tf.nn.sigmoid_cross_entropy_with_logits 
For brevity, let `x = logits`, `z = labels`.  The logistic loss is
    
          z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))
        = z * -log(1 / (1 + exp(-x))) + (1 - z) * -log(exp(-x) / (1 + exp(-x)))
        = z * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x)))
        = z * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x))
        = (1 - z) * x + log(1 + exp(-x))
        = x - x * z + log(1 + exp(-x))
'''

# 用pytorch来实现, 多标签二分类例子：
i = torch.rand(size=(3,4,5))
target = torch.randint(2, size=(3,4,5))
'''
In [20]: i
Out[20]:
tensor([[[0.0219, 0.8992, 0.9984, 0.0713, 0.5452],
         [0.2052, 0.3953, 0.5281, 0.0888, 0.7276],
         [0.4985, 0.3180, 0.9353, 0.0936, 0.5180],
         [0.5474, 0.7573, 0.0610, 0.9715, 0.0527]],

        [[0.8110, 0.2717, 0.1161, 0.9319, 0.4386],
         [0.8021, 0.7257, 0.6377, 0.9101, 0.8852],
         [0.0218, 0.2848, 0.6507, 0.1444, 0.0642],
         [0.9909, 0.2982, 0.8088, 0.9530, 0.0535]],

        [[0.6316, 0.5239, 0.5511, 0.7350, 0.1516],
         [0.1703, 0.7480, 0.9485, 0.8586, 0.0640],
         [0.3448, 0.3578, 0.8133, 0.9562, 0.7063],
         [0.0092, 0.3384, 0.7554, 0.2199, 0.6283]]])
   
In [22]: target
Out[22]:
tensor([[[0, 1, 0, 1, 0],
         [1, 1, 1, 0, 1],
         [0, 0, 0, 0, 0],
         [1, 1, 0, 0, 0]],

        [[0, 0, 0, 1, 0],
         [0, 0, 1, 0, 0],
         [0, 0, 0, 1, 0],
         [1, 1, 1, 0, 0]],

        [[0, 1, 0, 1, 1],
         [0, 0, 0, 0, 0],
         [1, 1, 1, 1, 1],
         [0, 0, 0, 1, 1]]])
'''
torch.nn.BCEWithLogitsLoss(reduce=False)(i, target.type(torch.float32))
'''
Out[26]:
tensor([[[0.7042, 0.3414, 1.3121, 0.6581, 1.0025],
         [0.5958, 0.5149, 0.4636, 0.7385, 0.3941],
         [0.9732, 0.8648, 1.2664, 0.7411, 0.9853],
         [0.4565, 0.3845, 0.7241, 1.2925, 0.7199]],

        [[1.1787, 0.8382, 0.7529, 0.3320, 0.9363],
         [1.1726, 1.1204, 0.4243, 1.2483, 1.2307],
         [0.7041, 0.8457, 1.0705, 0.6235, 0.7258],
         [0.3157, 0.5551, 0.3684, 1.2791, 0.7203]],

        [[1.0580, 0.4651, 1.0062, 0.3917, 0.6202],
         [0.7819, 1.1355, 1.2759, 1.2119, 0.7256],
         [0.5355, 0.5302, 0.3670, 0.3252, 0.4011],
         [0.6977, 0.8766, 1.1405, 0.5892, 0.4275]]])
'''
target*torch.log(torch.sigmoid(i)) +(1-target)*torch.log(1-torch.sigmoid(i))
'''
Out[27]:
tensor([[[-0.7042, -0.3414, -1.3121, -0.6581, -1.0025],
         [-0.5958, -0.5149, -0.4636, -0.7385, -0.3941],
         [-0.9732, -0.8648, -1.2664, -0.7411, -0.9853],
         [-0.4565, -0.3845, -0.7241, -1.2925, -0.7199]],

        [[-1.1787, -0.8382, -0.7529, -0.3320, -0.9363],
         [-1.1726, -1.1204, -0.4243, -1.2483, -1.2307],
         [-0.7041, -0.8457, -1.0705, -0.6235, -0.7258],
         [-0.3157, -0.5551, -0.3684, -1.2791, -0.7203]],

        [[-1.0580, -0.4651, -1.0062, -0.3917, -0.6202],
         [-0.7819, -1.1355, -1.2759, -1.2119, -0.7256],
         [-0.5355, -0.5302, -0.3670, -0.3252, -0.4011],
         [-0.6977, -0.8766, -1.1405, -0.5892, -0.4275]]])
'''
